{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"kaggle_check_lgbm.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMUndxSqC0eQuYYHvV/bntf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"nwBMhrrWHBLu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630908785024,"user_tz":-180,"elapsed":32068,"user":{"displayName":"Александр Гордеев","photoUrl":"","userId":"01480991716986255655"}},"outputId":"1b15cfbb-da48-4547-8395-d2f4c9a328ae"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/', force_remount=True)\n","!gdown --id 1s2ggWGqAjB-wlXkd6Pu7pM_3kqs6RhAF\n","!gdown --id 1iQCM32OzxqvmDsgL5j4et-yE5W8o6afE\n","!gdown --id 1bs8PHTExPfItW636-HVRVYwjjPilQVgy"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n","Downloading...\n","From: https://drive.google.com/uc?id=1s2ggWGqAjB-wlXkd6Pu7pM_3kqs6RhAF\n","To: /content/id_map.parquet\n","100% 1.20M/1.20M [00:00<00:00, 79.5MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1iQCM32OzxqvmDsgL5j4et-yE5W8o6afE\n","To: /content/train.csv\n","42.9MB [00:01, 24.2MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1bs8PHTExPfItW636-HVRVYwjjPilQVgy\n","To: /content/test.csv\n","24.8MB [00:00, 53.4MB/s]\n"]}]},{"cell_type":"code","metadata":{"id":"kM2Vr_FEHFrf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630908810655,"user_tz":-180,"elapsed":25636,"user":{"displayName":"Александр Гордеев","photoUrl":"","userId":"01480991716986255655"}},"outputId":"2b534416-392b-4a46-e6f3-5c373adb7e6b"},"source":["!pip install scikit-learn==0.24\n","!pip install tldextract\n","!pip install eli5\n","!pip install category_encoders\n","!pip install hyperopt"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting scikit-learn==0.24\n","  Downloading scikit_learn-0.24.0-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n","\u001b[K     |████████████████████████████████| 22.3 MB 2.0 MB/s \n","\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24) (1.4.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24) (1.19.5)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.24) (1.0.1)\n","Collecting threadpoolctl>=2.0.0\n","  Downloading threadpoolctl-2.2.0-py3-none-any.whl (12 kB)\n","Installing collected packages: threadpoolctl, scikit-learn\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","Successfully installed scikit-learn-0.24.0 threadpoolctl-2.2.0\n","Collecting tldextract\n","  Downloading tldextract-3.1.2-py2.py3-none-any.whl (87 kB)\n","\u001b[K     |████████████████████████████████| 87 kB 2.7 MB/s \n","\u001b[?25hRequirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tldextract) (2.23.0)\n","Collecting requests-file>=1.4\n","  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n","Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract) (3.0.12)\n","Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from tldextract) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from requests-file>=1.4->tldextract) (1.15.0)\n","Installing collected packages: requests-file, tldextract\n","Successfully installed requests-file-1.5.1 tldextract-3.1.2\n","Collecting eli5\n","  Downloading eli5-0.11.0-py2.py3-none-any.whl (106 kB)\n","\u001b[K     |████████████████████████████████| 106 kB 4.1 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from eli5) (1.19.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from eli5) (1.15.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from eli5) (2.11.3)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from eli5) (0.10.1)\n","Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from eli5) (0.24.0)\n","Requirement already satisfied: attrs>16.0.0 in /usr/local/lib/python3.7/dist-packages (from eli5) (21.2.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from eli5) (1.4.1)\n","Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from eli5) (0.8.9)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->eli5) (2.2.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->eli5) (1.0.1)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->eli5) (2.0.1)\n","Installing collected packages: eli5\n","Successfully installed eli5-0.11.0\n","Collecting category_encoders\n","  Downloading category_encoders-2.2.2-py2.py3-none-any.whl (80 kB)\n","\u001b[K     |████████████████████████████████| 80 kB 2.9 MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.24.0)\n","Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.10.2)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.19.5)\n","Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.5.1)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.4.1)\n","Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.1.5)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.1->category_encoders) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.1->category_encoders) (2.8.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.0.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->category_encoders) (2.2.0)\n","Installing collected packages: category-encoders\n","Successfully installed category-encoders-2.2.2\n","Requirement already satisfied: hyperopt in /usr/local/lib/python3.7/dist-packages (0.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from hyperopt) (1.15.0)\n","Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt) (3.12.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from hyperopt) (4.62.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt) (2.6.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from hyperopt) (1.19.5)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt) (0.16.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from hyperopt) (1.4.1)\n"]}]},{"cell_type":"code","metadata":{"id":"aSguHIa-HFpe"},"source":["import pandas as pd\n","import numpy as np\n","from scipy.stats import norm\n","from scipy import stats\n","from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n","from functools import partial\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","sns.set(style=\"whitegrid\")\n","\n","import time\n","from tqdm import tqdm_notebook\n","\n","from sklearn.model_selection import GridSearchCV, train_test_split\n","from sklearn.metrics import roc_auc_score, roc_curve\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import OrdinalEncoder\n","\n","import re\n","from wordcloud import WordCloud\n","from tldextract import extract\n","from gensim.models import Word2Vec\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import six\n","from pandas.api.types import is_sparse\n","from category_encoders import TargetEncoder"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l2OrsfuiHFm1"},"source":["from lightgbm import LGBMClassifier\n","from sklearn.model_selection import cross_val_score, GridSearchCV, TimeSeriesSplit\n","from numpy import mean\n","from numpy import std\n","from scipy.sparse import hstack\n","from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wv2uZu5UI-HY"},"source":["# Support"]},{"cell_type":"code","metadata":{"id":"1wpGxZR_HUcf"},"source":["def time_feature_extraction(data, train_flag):\n","\n","    data = data.assign(\n","        # session start time\n","        session_start=lambda x: x.filter(like='time').min(axis=1),\n","        # session end time\n","        session_end=lambda x: x.filter(like='time').max(axis=1),\n","        # session duration\n","        duration=lambda x: (x.session_end - x.session_start).dt.seconds,\n","        # start hour in a session\n","        start_hour=lambda x: x.session_start.apply(lambda x: x.hour).astype('int'),\n","        # day of week in a session\n","        weekday=lambda x: x.session_start.apply(lambda x: x.dayofweek).astype('category'),\n","\n","        # count_of_nans\n","        nans_count=lambda x: x.filter(like='webpage').isna().sum(axis=1),\n","        # nunique pages\n","        nunique_pages=lambda x: x.filter(like='webpage').apply(lambda row: row.nunique(), axis=1) / (10 - x.nans_count)\n","        )\n","\n","    data.loc[data[\"duration\"]==0, \"duration\"] = 0.001\n","    data[\"duration\"], lmbda = stats.boxcox(data.loc[:, \"duration\"])\n","\n","    data.loc[data[\"start_hour\"].isin([15,16,17,18,19,3]), \"start_hour\"] = 17\n","\n","    if train_flag:\n","        data = data.reset_index().sort_values(['session_start', \"session_id\"], ignore_index=True)\n","        data.drop(columns=[\"session_start\", \"session_end\"], inplace=True)\n","    else:\n","        data.drop(columns=[\"session_start\", \"session_end\"], inplace=True)\n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CQGrCvm9UlEg"},"source":["def intersection(train, id_map):\n","    \"seq of webpages\"\n","    data_train = merge_data_and_domain(train, id_map).loc[:, ['webpage_update%s' % i for i in range(1, 11)]+[\"start_hour\", \"weekday\"]]\n","\n","    data_train.loc[:, ['webpage_update%s' % i for i in range(1, 11)]] = data_train.loc[:, ['webpage_update%s' % i for i in range(1, 11)]].fillna(\"\")\n","    data_train.loc[:, \"string\"] = data_train.apply(lambda x: \"-\".join([x[i] for i in range(0,10) if x[i] != \"\"]), axis=1)\n","    data_train.loc[:, \"preproc_tokens\"] = data_train.loc[:, \"string\"].apply(lambda x: x.split('-')).values\n","\n","    data_train = data_train.loc[:, [\"preproc_tokens\"]]\n","\n","    data_train.loc[:, [\".youtube\", \"sn gxouxg jqbe.googlevideo\", \".ytimg\"]] = 0\n","\n","    for token in [\".youtube\", \"sn gxouxg jqbe.googlevideo\", \".ytimg\"]:\n","        data_train[token] = data_train.loc[:, \"preproc_tokens\"].apply(lambda x: 1 if token in x else 0)\n","    data_train[\"intersection\"] = 0\n","    data_train.loc[(data_train[\".youtube\"]==1) & (data_train[\"sn gxouxg jqbe.googlevideo\"]==1) & (data_train[\".ytimg\"]==1), \"intersection\"] = 1\n","\n","    train = pd.concat([train, data_train.loc[:, [\"intersection\"]]], axis=1)\n","\n","    return train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mfshDua3Xh7j"},"source":["def text_preprocessing_1(text):\n","    \"\"\"text_preproc\"\"\"\n","    # split text by . or -\n","    clean_text = re.split(\"\\.|-\", text)\n","    # join text\n","    clean_text = \" \".join(clean_text)\n","    # remove nums\n","    clean_text = re.sub(\"[0-9]\", \"\", clean_text)\n","    # remove spaces\n","    clean_text = re.sub(' +', ' ', clean_text)\n","    # split\n","    clean_text = re.split(\" \", clean_text)\n","    # seq must be longer then 1\n","    clean_text = [x for x in clean_text if len(x)>1]\n","    # join text\n","    clean_text = \" \".join(clean_text)\n","\n","    return clean_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wOCnDGRFgESu"},"source":["def merge_data_and_domain(data, id_map):\n","    \"\"\"merge sub_domain + domain to full webapage\"\"\"\n","    for i in range(1,11):\n","        data = pd.merge(data, id_map.rename(columns={\"id\":\"webpage\"+str(i), \n","                                                     \"webpage_update\":\"webpage_update\"+str(i)}).loc[:, [\"webpage\"+str(i),\n","                                                                                                        \"webpage_update\"+str(i)]], \n","                        on=\"webpage\"+str(i), how=\"left\")\n","    return data\n","    \n","def tf_idf_features(train, test, vectorizer_params):\n","    \"\"\"apply tf-idf to sub_domain + domain\"\"\"\n","\n","    webpages_update = ['webpage_update%s' % i for i in range(1, 11)]\n","\n","    train_sessions = train.loc[:, webpages_update].fillna(\"\")\n","    test_sessions = test.loc[:, webpages_update].fillna(\"\")\n","\n","    train_tokens = train_sessions.apply(lambda x: \"-\".join([x[i] for i in range(0,10) if x[i] != \"\"]), axis=1)\n","    test_tokens = test_sessions.apply(lambda x: \"-\".join([x[i] for i in range(0,10) if x[i] != \"\"]), axis=1)\n","\n","    vectorizer = TfidfVectorizer(**vectorizer_params)\n","    X_train = vectorizer.fit_transform(train_tokens)\n","    X_test = vectorizer.transform(test_tokens)\n","    \n","    return X_train, X_test, vectorizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3mrE9U3SANCp"},"source":["def sparse_to_dense(x_train, x_test):\n","    \"\"\"sparse matrix to normal dense\"\"\"\n","    for column in x_train.columns:\n","        if is_sparse(x_train[column]):\n","            x_train.loc[:, column] = x_train.loc[:, column].sparse.to_dense()\n","\n","    for column in x_test.columns:\n","        if is_sparse(x_test[column]):\n","            x_test.loc[:, column] = x_test.loc[:, column].sparse.to_dense()\n","    return x_train, x_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-aZxwxxFRHoD"},"source":["def extraction(train, x_test, id_map, vectorizer_params, \n","               time_features, popular_features, \n","               columns_tf_idf, tf_idf, ohe_flag):\n","\n","    if time_features:      \n","        print(\"extract time features\")    \n","        train = time_feature_extraction(train, train_flag=True)\n","        x_test = time_feature_extraction(x_test, train_flag=False)\n","\n","    if popular_features:\n","        print(\"intersection features\") \n","        train = intersection(train.copy(), id_map.copy())\n","        x_test = intersection(x_test, id_map)\n","\n","    if tf_idf:\n","        print(\"apply tf idf\")  \n","        data_train = merge_data_and_domain(train, id_map).loc[:, ['webpage_update%s' % i for i in range(1, 11)]]\n","        data_test = merge_data_and_domain(x_test, id_map).loc[:, ['webpage_update%s' % i for i in range(1, 11)]]\n","        train_tf_idf, test_tf_idf, vectorizer = tf_idf_features(train=data_train, test=data_test, vectorizer_params=vectorizer_params)\n","        train_tf_idf = pd.DataFrame.sparse.from_spmatrix(train_tf_idf, \n","                                                         columns=vectorizer.get_feature_names(), \n","                                                         index=train.set_index(\"session_id\").index)\n","        train_tf_idf = train_tf_idf.loc[:, columns_tf_idf]\n","        test_tf_idf = pd.DataFrame.sparse.from_spmatrix(test_tf_idf, \n","                                                        columns=vectorizer.get_feature_names(), \n","                                                        index=x_test.set_index(\"session_id\").index)\n","        test_tf_idf = test_tf_idf.loc[:, columns_tf_idf]\n","\n","    # set_index\n","    train.set_index(\"session_id\", inplace=True)\n","    x_test.set_index(\"session_id\", inplace=True)\n","\n","    x_train, y_train = train.drop(columns=[\"target\"]), train.loc[:, [\"target\"]]\n","\n","    # drop useless features\n","    other_features_list = [\"time\"+str(i) for i in range(1,11)]\n","    x_train.drop(columns=other_features_list, inplace=True)\n","    x_test.drop(columns=other_features_list, inplace=True)\n","\n","    if ohe_flag:\n","        encoder = OneHotEncoder(handle_unknown=\"ignore\")\n","        features_dummy_train = encoder.fit_transform(x_train.loc[:, [\"weekday\"]])\n","        features_dummy_train = pd.DataFrame.sparse.from_spmatrix(features_dummy_train, index=x_train.index)\n","        features_dummy_test = encoder.transform(x_test.loc[:, [\"weekday\"]])\n","        features_dummy_test = pd.DataFrame.sparse.from_spmatrix(features_dummy_test, index=x_test.index)\n","\n","        cat_columns = list(range(features_dummy_test.shape[1]))\n","\n","        x_train.drop(columns=[\"weekday\"], inplace=True)\n","        x_test.drop(columns=[\"weekday\"], inplace=True)\n","\n","        x_train = pd.concat([features_dummy_train, x_train], axis=1)\n","        x_test = pd.concat([features_dummy_test, x_test], axis=1)\n","\n","    columns = x_train.columns\n","\n","    if tf_idf:\n","        x_train = pd.concat([x_train, train_tf_idf], axis=1)\n","        x_test = pd.concat([x_test, test_tf_idf], axis=1)\n","        x_train, x_test = sparse_to_dense(x_train, x_test)\n","        return x_train, y_train, x_test, vectorizer, columns, cat_columns\n","    else:\n","        x_train, x_test = sparse_to_dense(x_train, x_test)\n","        return x_train, y_train, x_test, columns, cat_columns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jS2_wRfeI8FI"},"source":["def save_submission(pred, number):\n","    pd.Series(\n","        pred, name='target', index=pd.Index(range(len(pred)), name='session_id')\n","    ).to_csv('/content/gdrive/MyDrive/EPAM/Week 7. Trees/HW/submissions_test/notebook_submission' + str(number) + '.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ua32wYYtB3ul"},"source":["def objective(space, estimator, x_train, y_train):\n","    time_split = TimeSeriesSplit(n_splits=7)\n","\n","    params = {\n","        'learning_rate': space['learning_rate'],\n","        'reg_alpha': space['reg_alpha'],\n","        'reg_lambda': space['reg_lambda'],\n","        'num_leaves': int(space['num_leaves']),\n","        'n_estimators': int(space['n_estimators']),\n","        'min_child_samples': int(space['min_child_samples']),\n","    }\n","    \n","    # задаём модели требуемые параметры    \n","    estimator.set_params(**params)\n","    \n","    score = cross_val_score(estimator, x_train, y_train, scoring='roc_auc', cv=time_split).mean()\n","    print(\"AUC {:.3f} params {}\".format(score, params))\n","    return {'loss':1 - score, 'status': STATUS_OK }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eBaAwdJnJVaS"},"source":["def df_results(hp_results):\n","    \"\"\"\n","    Отображаем результаты hyperopt в формате DataFrame \n","\n","    :hp_results: результаты hyperop\n","    :return: pandas DataFrame\n","    \"\"\" \n","\n","    results = pd.DataFrame([{**x, **x['params']} for x in  hp_results])\n","    results.drop(labels=['status', 'params'], axis=1, inplace=True)\n","    results.sort_values(by=['loss'], ascending=False, inplace=True)\n","    return results"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gxi49aqFJCKM"},"source":["# Preprocessing"]},{"cell_type":"code","metadata":{"id":"SzrL_ykapul2"},"source":["# usefull features tf-idf\n","list_positive = ['.express', '.info jeunes', '.vk', '.melty', 'fr web img.acsta',\n","                '.videostep', 'khms.google', '.radio canada', 'api.bing',\n","                '.banque chalus', '.audienceinsights', 'demotivateur.disqus',\n","                '.indeed', 'media.melty', '.video', '.jobisjob', '.blogger',\n","                'gujynbgx.admedia', '.youwatch', 'reviewer.lavoixdunord',\n","                'img wikia.nocookie', 'static.programme tv', 'static.flickr',\n","                '.audienceinsights facebook', '.regarder film gratuit', '.bbc',\n","                'facebook .audienceinsights', 'static.videostep', '.exashare',\n","                '.blastr', 'docs.google', 'graphics.nytimes', '.reddit']\n","\n","list_neg = ['.futura sciences', 'clients.google clients.google clients.google',\n","            'fr mg mail.yahoo', '.ztat', 'fbstatic.akamaihd', '.spin',\n","            'facebook', 'ba.commentcamarche', 'static.ccm', 'safebrowsing cache.google', \n","            '.linkedin', '.wordreference', 'fr.openclassrooms', '.bing', \n","            'clients.google clients.google', 'mail.google', 'plus.google']\n","\n","features_list = list_positive + list_neg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DgK9w3KCHRgn"},"source":["OUTPUT_FOLDER = \"/content/gdrive/MyDrive/EPAM/Week 7. Trees/HW/model_test\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fWiLFHmJHRbm"},"source":["train = pd.read_csv(\"./train.csv\", index_col=0, parse_dates=[f'time{i+1}' for i in range(10)])\n","x_test = pd.read_csv('./test.csv', index_col=0, parse_dates=[f'time{i+1}' for i in range(10)])\n","x_test.reset_index(inplace=True)\n","id_map = pd.read_parquet(\"/content/id_map.parquet\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Lo5TGDZHcBF"},"source":["# get user's webpages \n","user_frame = train.loc[train[\"target\"]==1].reset_index(drop=True)\n","\n","array = np.array([])\n","for element in [\"webpage\"+str(x) for x in range(1,11)]:\n","    unique_array = user_frame.loc[:, element].unique()\n","    array = np.concatenate([array, unique_array])\n","array = {x for x in array if x==x}\n","\n","id_map[\"target\"] = 0\n","id_map.loc[id_map[\"id\"].isin(array), \"target\"] = 1\n","\n","# extract \"sub-domain\", \"domain\", \"suf\" from url of webpage\n","list_ = [list(extract(id_map.loc[i].webpage)) for i in id_map.index]\n","id_map.loc[:, [\"sub\",\"domain\",\"suf\"]] = list_\n","\n","# url preprocessing\n","id_map.loc[:, \"domain\"] = [text_preprocessing_1(text=x) for x in id_map.loc[:, \"domain\"].values]\n","id_map.loc[:, \"sub\"] = [text_preprocessing_1(text=x) for x in id_map.loc[:, \"sub\"].values]\n","id_map.loc[:, \"sub\"] = id_map.loc[:, \"sub\"].apply(lambda x: \"\" if x in [\"www\", \"js init\"] else x)\n","id_map[\"webpage_update\"] = id_map.apply(lambda x: \".\".join([x[3], x[4]]), axis=1)\n","id_map.loc[:, \"webpage_update\"] = id_map.loc[:, \"webpage_update\"].apply(lambda x: \"twitter\" if x in [\".twitter\", \"platform.twitter\"] else x)\n","id_map.loc[:, \"webpage_update\"] = id_map.loc[:, \"webpage_update\"].apply(lambda x: \"facebook\" if x in [\"static ak.facebook\", \".facebook\", \"connect.facebook\"] else x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gxiLalpPh9oi"},"source":["# params for tf-idf\n","vectorizer_params={'ngram_range': (1, 5), 'max_features': 30000,\n","                   \"min_df\": 2, \"max_df\": 0.7,\n","                   'tokenizer': lambda s: s.split(\"-\")\n","                   }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"24LGuAJ24KrJ","executionInfo":{"status":"ok","timestamp":1630922010942,"user_tz":-180,"elapsed":104396,"user":{"displayName":"Александр Гордеев","photoUrl":"","userId":"01480991716986255655"}},"outputId":"8f5bd0dc-3e18-40c7-da31-c3f0ae2e3b1f"},"source":["# time_features + popular_features\n","x_train3, y_train3, x_test3, vectorizer3, columns3, cat_columns3 = extraction(train=train.copy(), x_test=x_test.copy(), id_map=id_map, \n","                                                                   vectorizer_params=vectorizer_params, time_features=True,\n","                                                                   popular_features=True, columns_tf_idf=features_list,\n","                                                                   tf_idf=True, ohe_flag=True)\n","\n","x_train3.fillna(0, inplace=True)\n","y_train3.fillna(0, inplace=True)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["extract time features\n","intersection features\n","apply tf idf\n"]}]},{"cell_type":"markdown","metadata":{"id":"f3gXkvTb5eGW"},"source":["## LGBMClassifier"]},{"cell_type":"code","metadata":{"id":"ymWGDwIvmv_i"},"source":["estimator = LGBMClassifier(random_state=123, is_unbalance=True)\n","\n","space = {\n","    'learning_rate': hp.loguniform('learning_rate', low=-4*np.log(10), high=-1*np.log(10)),\n","    'reg_alpha': hp.loguniform('reg_alpha', -4*np.log(10), 2*np.log(10)),\n","    'reg_lambda': hp.loguniform('reg_lambda', -4*np.log(10), 2*np.log(10)),\n","    'num_leaves': hp.uniform('num_leaves', 10, 80),\n","    'n_estimators': hp.uniform('n_estimators', 100, 1000),\n","    'min_child_samples': hp.uniform('min_child_samples', 10, 200),\n","    }\n","\n","trials = Trials()\n","\n","best = fmin(\n","    # функция для оптимизации \n","    fn=partial(objective, estimator=estimator, x_train=x_train1, y_train=y_train1),\n","    # пространство поиска гиперпараметров\n","    space=space,\n","    # алгоритм поиска\n","    algo=tpe.suggest,\n","    # число итераций\n","    max_evals=450,\n","    # куда сохранять историю поиска\n","    trials=trials,\n","    # random state\n","    rstate=np.random.RandomState(1),\n","    # progressbar\n","    show_progressbar=True\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kkzgt_H6HIZw","executionInfo":{"status":"ok","timestamp":1630915194939,"user_tz":-180,"elapsed":66149,"user":{"displayName":"Александр Гордеев","photoUrl":"","userId":"01480991716986255655"}},"outputId":"82706880-c304-4954-df7e-79149868df42"},"source":["time_split = TimeSeriesSplit(n_splits=7)\n","\n","estimator_3 = LGBMClassifier(boosting_type=\"goss\",  class_weight='balanced',\n","                             min_child_samples=115, n_estimators=400, num_leaves=32,\n","                             random_state=42, reg_alpha=0.25, reg_lambda=0.25)\n","\n","score = cross_val_score(estimator_3, x_train3, y_train3, scoring='roc_auc', cv=time_split).mean()\n","estimator_3.fit(x_train3, y_train3)\n","print(\"score: \", score)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["score:  0.9145039288638277\n"]}]},{"cell_type":"code","metadata":{"id":"TxIYNzOulrJI"},"source":["# predict\n","lgbm_pred = estimator_3.predict_proba(x_test3)[:, 1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5FvUZgkdobW5"},"source":["# save Submission\n","save_submission(lgbm_pred, number=47)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nQ2JVbAAljfo"},"source":[""],"execution_count":null,"outputs":[]}]}